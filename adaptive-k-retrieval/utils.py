import tiktoken
from openai import OpenAI
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import torch
import os
from typing import List, Tuple, Dict, Optional, Any
from dotenv import load_dotenv
import numpy as np

load_dotenv()

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])


LlamaMessagesType = List[Dict[str, str]]


def tensor_to_serializable(obj: Any) -> Any:
    """Convert tensors to serializable objects.
    This is for saving the results to JSON.
    Args:
        obj (Any): Object to convert.
    Returns:
        Any: Converted object.
    """
    if torch.is_tensor(obj):
        return obj.tolist()
    if isinstance(obj, np.float32):
        # This is extremely important to avoid `ValueError: Circular reference detected` when json.dumps
        return float(obj)
    return obj


def find_circular_references(obj: Any,
                             path: str = "root",
                             seen: Optional[Dict[int, str]] = None):
    """For testing circular references in the JSON object.
    
    Args:
        obj (Any): Object to check.
        path (str): Current path in the object.
        seen (Optional[Dict[int, str]]): Seen object IDs.
    Returns:
        None
    """
    if seen is None:
        seen = {}

    obj_id = id(obj)
    
    if obj_id in seen:
        print(f"Circular reference detected at: {path}")
        print(f"Object ID: {obj_id}")
        print(f"Object type: {type(obj)}")
        print(f"Object value: {obj}")
        return

    seen[obj_id] = path

    if isinstance(obj, dict):
        for k, v in obj.items():
            find_circular_references(v, f"{path}[{repr(k)}]", seen)        
    elif isinstance(obj, list):
        for idx, item in enumerate(obj):
            find_circular_references(item, f"{path}[{idx}]", seen)
    elif isinstance(obj, tuple):
        for idx, item in enumerate(obj):
            find_circular_references(item, f"{path}({idx})", seen)
    elif isinstance(obj, set):
        for idx, item in enumerate(obj):
            find_circular_references(item, f"{path}{{{idx}}}", seen)
    # You can expand with other container types as needed (e.g., custom classes)


def count_llama_tokens(model: str,
                       outputs: List[Dict[str, LlamaMessagesType]]) -> Tuple[int, int]:
    """Count the number of tokens generated by Llama.

    Args:
        model (str): Model name.
        outputs (List[Dict[str, LlamaMessagesType]]): Outputs from Llama.

    Returns:
        Tuple[int, int]: Number of input tokens, number of output tokens.
    """
    tokenizer = AutoTokenizer.from_pretrained(model)
    # print(outputs)
    input_messages = [m["content"] for m in outputs[0]
                      ["generated_text"] if m["role"] in {"user", "system"}]
    output_messages = [m["content"] for m in outputs[0]
                       ["generated_text"] if m["role"] == "assistant"]
    input_tokens = [tokenizer.tokenize(text) for text in input_messages]
    output_tokens = [tokenizer.tokenize(text) for text in output_messages]
    input_num_tokens = sum([len(token) for token in input_tokens])
    output_num_tokens = sum([len(token) for token in output_tokens])

    return input_num_tokens, output_num_tokens


def num_tokens_from_string(string: str,
                           encoding_name: Optional[str] = None,
                           model_name: Optional[str] = None) -> int:
    """Returns the number of tokens contained
    in a text string (by OpenAI).
    
    Args:
        string (str): The input string.
        encoding_name (Optional[str]): The encoding name.
        model_name (Optional[str]): The model name.
            If specified, the encoding name is inferred from the model name.
    Returns:
        int: The number of tokens.
    """
    if encoding_name is None:
        if model_name is not None:
            encoding = tiktoken.encoding_for_model(model_name)
        else:
            raise ValueError(
                "Either `encoding_name` or `model_name` must be specified.")
    else:
        encoding = tiktoken.get_encoding(encoding_name)

    return len(encoding.encode(string))


class Embedding:
    def __init__(self,
                 model: str,
                 hf: bool = False,
                 use_sentence_transformer: bool = True,
                 device: str = "cpu"):
        """Class for generating embeddings.
        Args:
            model (str): The model name or path.
            hf (bool): If true, an embedding model is loaded through transformers.
            use_sentence_transformer (bool): If true, use SentenceTransformer.
                Otherwise, use AutoModel from transformers.
            device (str): The device to use.
        """
        self.hf = hf
        self.use_sentence_transformer = use_sentence_transformer
        self.device = device

        if hf:
            if use_sentence_transformer:
                self.model = SentenceTransformer(model).to(device)
            else:
                self.tokenizer = AutoTokenizer.from_pretrained(model)
                self.model = AutoModel.from_pretrained(model)
                self.model.eval()
        else:
            self.model = model

    def get_embedding(self,
                      text: str | list,
                      batch_size: int = 1,
                      convert_to_numpy: bool = False) -> List[float]:
        """Get an embedding.
        
        Args:
            text (str | list): text to encode.
            batch_size (int): batch size for encoding sentence embeddings.
            convert_to_numpy (bool): If true, convert the output to numpy array.
            
        Returns:
            List[float]: embedding.
        """
        if self.hf:
            embedding = self.get_embedding_from_hf(text,
                                                   batch_size,
                                                   convert_to_numpy=convert_to_numpy)
        else:
            embedding = self.get_embedding_from_openai(text, self.model)
            
        return embedding

    def get_embedding_from_openai(self,
                                  text: str | list,
                                  model: str = "text-embedding-3-small") -> List[float]:
        """Get an embedding for a chunk (use OpenAI).
        
        Args:
            text (str | list): text to encode.
            model (str): model name.
        Returns:
            List[float]: embedding.
        """
        if isinstance(text, str):
            text = [text.replace("\n", " ")]
        elif isinstance(text, list):
            text = [t.replace("\n", " ") for t in text]
        else:
            raise TypeError(
                f"`text` is supposed to be str or list, but {type(text)} found.")
        result = client.embeddings.create(
            input=text,
            model=model
        )
        return result.data[0].embedding

    def get_embedding_from_hf(self,
                              text: str | list,
                              batch_size: int = 1,
                              convert_to_numpy: bool = False) -> List[float]:
        """Get embedding from huggingface.
        For more information, see https://huggingface.co/BAAI/bge-large-en-v1.5
        Here we use transformers to extract the embedding,
        but you may also use `sentence_transformers`.

        Args:
            text (str | list): text to encode.
            batch_size (int): batch size for encoding sentence embeddings.
            convert_to_numpy (bool): If true, convert the output to numpy array.
        Returns:
            List[float]: embedding.
        """
        # Make sure that self.hf is True
        assert self.hf, "This class was not instantiated with the hf mode."

        if isinstance(text, str):
            text = text.split("\n")
        elif isinstance(text, list):
            pass
        else:
            raise TypeError(
                f"`text` is supposed to be str or list, but {type(text)} found.")

        if self.use_sentence_transformer:
            convert_to_tensor = False if convert_to_numpy else True
            return self.model.encode(text,
                                     batch_size=batch_size,
                                     convert_to_numpy=convert_to_numpy,
                                     convert_to_tensor=convert_to_tensor,
                                     show_progress_bar=False,
                                     device=self.device)

        else:
            encoded_input = self.tokenizer(text,
                                           padding=True,
                                           truncation=True,
                                           return_tensors="pt")
            # for s2p (short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)
            # encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')
            # for debug:
            # print(encoded_input)

            # Compute token embeddings
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                # Perform pooling; in this case, CLS pooling.
                sent_emb = model_output[0][:, 0]

            # normalize
            sent_emb = torch.nn.functional.normalize(sent_emb, p=2, dim=1)
            # print(f"Number of sent_emb: {len(sent_emb)}")
            return sent_emb
